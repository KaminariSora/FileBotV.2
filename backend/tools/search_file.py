from langchain.tools import tool
import os
from pathlib import Path
import os
from langchain.chat_models import init_chat_model


@tool(description="Search for files based on user intent dictionary.")
def search_file(
    content: str,
    search_path: str,
    type: str = None
) -> str:
    search_dir = Path(search_path)
    results = []

    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "./service/service_account.json"

    llm = init_chat_model(
        "gemini-2.0-flash",
        model_provider="google_genai",
        temperature=0.8
    )

    def chunk_text(text, chunk_size=2000):
        """‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•"""
        for i in range(0, len(text), chunk_size):
            yield text[i:i+chunk_size]

    def is_readable_file(file_path):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        readable_extensions = {'.txt', '.md', '.py', '.js', '.html', '.css', '.json', 
                              '.xml', '.csv', '.log', '.sql', '.yml', '.yaml', '.ini', 
                              '.conf', '.properties', '.sh', '.bat', '.dockerfile'}
        return file_path.suffix.lower() in readable_extensions

    def get_file_encoding(file_path):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö encoding ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        encodings = ['utf-8', 'utf-8-sig', 'cp874', 'windows-1252', 'iso-8859-1']
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    f.read(100)
                return encoding
            except (UnicodeDecodeError, UnicodeError):
                continue
        return None

    def extract_relevant_content(file_content, user_content, max_context=1000):
        """‡πÅ‡∏¢‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå"""
        lines = file_content.split('\n')
        relevant_sections = []
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        keywords = user_content.lower().split()
        for i, line in enumerate(lines):
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in keywords):
                # ‡πÄ‡∏≠‡∏≤‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏£‡∏≠‡∏ö‡πÜ ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö
                start = max(0, i - 3)
                end = min(len(lines), i + 4)
                context = '\n'.join(lines[start:end])
                relevant_sections.append(context)
        
        # ‡∏£‡∏ß‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß
        combined = '\n...\n'.join(relevant_sections[:3])  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 3 ‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å
        if len(combined) > max_context:
            combined = combined[:max_context] + "..."
        
        return combined if combined else file_content[:max_context]

    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
    for file_path in search_dir.rglob("*"):
        if not file_path.is_file():
            continue

        # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
        if type and not file_path.name.lower().endswith(type.lower()):
            continue

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not is_readable_file(file_path):
            continue

        try:
            # ‡∏´‡∏≤ encoding ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
            encoding = get_file_encoding(file_path)
            if not encoding:
                continue

            # ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
            with open(file_path, "r", encoding=encoding) as f:
                file_content = f.read()

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÉ‡∏´‡πâ‡πÅ‡∏¢‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô
            if len(file_content) > 5000:
                relevant_content = extract_relevant_content(file_content, content)
            else:
                relevant_content = file_content

            # ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡πÜ ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
            matched = False
            for chunk in chunk_text(relevant_content):
                if not chunk.strip():  # ‡∏Ç‡πâ‡∏≤‡∏° chunk ‡∏ß‡πà‡∏≤‡∏á
                    continue
                    
                prompt = f"""‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö '{content}' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                
                    ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå:
                    {chunk}

                    ‡∏ï‡∏≠‡∏ö 'yes' ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏´‡∏£‡∏∑‡∏≠ 'no' ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏™‡∏±‡πâ‡∏ô‡πÜ"""

                try:
                    response = llm.invoke(prompt)
                    response_text = response.content.strip().lower()
                    
                    if "yes" in response_text:
                        results.append({
                            'file_path': str(file_path),
                            'file_name': file_path.name,
                            'file_size': file_path.stat().st_size,
                            'matched_content': chunk[:200] + "..." if len(chunk) > 200 else chunk
                        })
                        matched = True
                        break  # ‡∏´‡∏¢‡∏∏‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö chunk ‡∏≠‡∏∑‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
                        
                except Exception as e:
                    print(f"Error processing chunk from {file_path}: {e}")
                    continue

        except Exception as e:
            print(f"Error reading file {file_path}: {e}")
            continue

    # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    if results:
        response_text = f"‡∏û‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(results)} ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö '{content}':\n\n"
        
        for i, result in enumerate(results, 1):
            file_size_kb = result['file_size'] / 1024
            response_text += f"{i}. {result['file_name']}\n"
            response_text += f"   üìÅ Path: {result['file_path']}\n"
            response_text += f"   üìä Size: {file_size_kb:.1f} KB\n"
            response_text += f"   üìÑ Preview: \n{result['matched_content']}\n\n"
        
        return response_text
    else:
        return f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö '{content}' ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå {search_path}"